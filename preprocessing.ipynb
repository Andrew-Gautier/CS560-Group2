{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b10a4202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a394bed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0       id          created_utc ticker subreddit  \\\n",
      "0           0  1oyzy4v  2025-11-16 22:59:25   nvda    stocks   \n",
      "1           1  1owabf1  2025-11-13 19:23:30   nvda    stocks   \n",
      "2           2  1ow7lym  2025-11-13 17:43:31   nvda    stocks   \n",
      "3           3  1ovqca4  2025-11-13 03:17:50   nvda    stocks   \n",
      "4           4  1ovctdu  2025-11-12 18:15:41   nvda    stocks   \n",
      "\n",
      "                 author                                              title  \\\n",
      "0            Denver-Ski  Peter Thielâ€™s latest 13F shows a full exit fro...   \n",
      "1        SecretComposer  Do you have any less discussed positions that ...   \n",
      "2    Necessary_Fold5478  Nvidia CEO reportedly said China will lead the...   \n",
      "3  Apprehensive_Two1528  USD to CNY hits lowest level YTD. Apple is gon...   \n",
      "4           One_Rub7972  CRWV has plummeted; is now a good time to buy ...   \n",
      "\n",
      "   score  upvote_ratio  num_comments query   stock                 created_dt  \\\n",
      "0   1478          0.93           241  nvda  nvidia  2025-11-16 22:59:25+00:00   \n",
      "1     73          0.96           144  nvda  nvidia  2025-11-13 19:23:30+00:00   \n",
      "2     89          0.74           129  nvda  nvidia  2025-11-13 17:43:31+00:00   \n",
      "3      9          0.63             7  nvda  nvidia  2025-11-13 03:17:50+00:00   \n",
      "4     96          0.77           135  nvda  nvidia  2025-11-12 18:15:41+00:00   \n",
      "\n",
      "   days_ago  weeks_ago  \n",
      "0         1          0  \n",
      "1         4          0  \n",
      "2         4          0  \n",
      "3         5          0  \n",
      "4         5          0  \n",
      "        Price               Close             Close.1             Close.2  \\\n",
      "0      Ticker                AAPL                AMZN               GOOGL   \n",
      "1        Date                 NaN                 NaN                 NaN   \n",
      "2  2024-11-18  226.99337768554688   201.6999969482422   174.5303192138672   \n",
      "3  2024-11-19    227.252197265625  204.61000061035156  177.33792114257812   \n",
      "4  2024-11-20   227.9689483642578   202.8800048828125   175.2073211669922   \n",
      "\n",
      "              Close.3             Close.4            Close.5  \\\n",
      "0                MSFT                NVDA               TSLA   \n",
      "1                 NaN                 NaN                NaN   \n",
      "2   412.6619567871094  140.10980224609375   338.739990234375   \n",
      "3  414.67681884765625   146.9678497314453              346.0   \n",
      "4   412.3939208984375  145.84817504882812  342.0299987792969   \n",
      "\n",
      "                 High             High.1              High.2  ...    Volume.2  \\\n",
      "0                AAPL               AMZN               GOOGL  ...       GOOGL   \n",
      "1                 NaN                NaN                 NaN  ...         NaN   \n",
      "2  228.70563485331775  204.6699981689453   174.6697039128958  ...  20135300.0   \n",
      "3    229.123737666772  205.3000030517578  178.08462810700078  ...  23434900.0   \n",
      "4  228.89475383274825  203.1300048828125  176.89985394642653  ...  18997100.0   \n",
      "\n",
      "     Volume.3     Volume.4     Volume.5 days_ago days_ago.1 days_ago.2  \\\n",
      "0        MSFT         NVDA         TSLA     AAPL       AMZN      GOOGL   \n",
      "1         NaN          NaN          NaN      NaN        NaN        NaN   \n",
      "2  24727000.0  221205300.0  126085600.0      365        365        365   \n",
      "3  18133500.0  227834900.0   88852500.0      364        364        364   \n",
      "4  19191700.0  309871700.0   66340700.0      363        363        363   \n",
      "\n",
      "  days_ago.3 days_ago.4 days_ago.5  \n",
      "0       MSFT       NVDA       TSLA  \n",
      "1        NaN        NaN        NaN  \n",
      "2        365        365        365  \n",
      "3        364        364        364  \n",
      "4        363        363        363  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "reddit_df = pd.read_csv('year_reddit_data.csv')\n",
    "yf_df = pd.read_csv('year_yf_data.csv')\n",
    "\n",
    "print(reddit_df.head())\n",
    "print(yf_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c2873",
   "metadata": {},
   "source": [
    "# Reddit + Yahoo Finance preprocessing pipeline\n",
    "\n",
    "This notebook prepares weekly sequences for the BiLSTM self-attention model.\n",
    "\n",
    "Steps:\n",
    "- Normalize schemas and dates on `reddit_df` and `yf_df`.\n",
    "- Clean text (remove URLs, whitespace) and detect ticker mentions.\n",
    "- Compute FinBERT embeddings and sentiment per post.\n",
    "- Aggregate per ticker-day: engagement-weighted embedding and sentiment/engagement features.\n",
    "- Join OHLCV from Yahoo Finance data.\n",
    "- Build 5-day windows and next-week labels (direction/magnitude).\n",
    "- Create a WeeklySentimentDataset and collate a batch.\n",
    "- Run the batch through `LSTMClassifier` to verify shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39782ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"https?://\\S+\", \" \", text)\n",
    "    text = re.sub(r\"\\n+\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "\n",
    "def load_finbert(device):\n",
    "    tok = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"ProsusAI/finbert\", output_hidden_states=True\n",
    "    ).to(device)\n",
    "    mdl.eval()\n",
    "    return tok, mdl\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def finbert_embed_and_sentiment(texts, tokenizer, model, device, batch_size: int = 16):\n",
    "    all_embeds = []\n",
    "    all_scores = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        chunk = texts[i : i + batch_size]\n",
    "        enc = tokenizer(chunk, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        out = model(**enc)\n",
    "        hidden = out.hidden_states[-1]\n",
    "        mask = enc[\"attention_mask\"].unsqueeze(-1)\n",
    "        masked = hidden * mask\n",
    "        lengths = mask.sum(dim=1).clamp_min(1)\n",
    "        mean_pool = masked.sum(dim=1) / lengths\n",
    "        logits = out.logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        score = probs[:, 2] - probs[:, 0]\n",
    "        all_embeds.append(mean_pool.cpu())\n",
    "        all_scores.append(score.cpu())\n",
    "    return torch.cat(all_embeds, dim=0), torch.cat(all_scores, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce61c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize schemas\n",
    "reddit = reddit_df.copy()\n",
    "yf = yf_df.copy()\n",
    "\n",
    "# Reddit expected columns: ['post.id','post.created_utc','post.title','post.selftext','post.ups','post.downs','post.upvote_ratio','post.num_comments','post.score','Post.subreddit.display_name','Ticker_id']\n",
    "# Map to simpler names if present\n",
    "col_map = {\n",
    "    'post.id': 'post_id',\n",
    "    'post.created_utc': 'created_utc',\n",
    "    'post.title': 'title',\n",
    "    'post.selftext': 'selftext',\n",
    "    'post.ups': 'ups',\n",
    "    'post.downs': 'downs',\n",
    "    'post.upvote_ratio': 'upvote_ratio',\n",
    "    'post.num_comments': 'num_comments',\n",
    "    'post.score': 'score',\n",
    "    'Post.subreddit.display_name': 'subreddit',\n",
    "    'Ticker_id': 'ticker'\n",
    "}\n",
    "reddit.rename(columns={k: v for k, v in col_map.items() if k in reddit.columns}, inplace=True)\n",
    "\n",
    "# Parse datetime and clean text\n",
    "if 'created_utc' in reddit.columns:\n",
    "    reddit['created_utc'] = pd.to_datetime(reddit['created_utc'], utc=True, errors='coerce')\n",
    "else:\n",
    "    raise ValueError('created_utc column not found in reddit_df')\n",
    "\n",
    "# Use only post titles (selftext removed from model inputs)\n",
    "reddit['title'] = reddit.get('title', '').astype(str)\n",
    "reddit['text'] = reddit['title'].map(clean_text)\n",
    "\n",
    "# Ensure numeric engagement columns exist\n",
    "for c, dtp, default in [\n",
    "    ('ups', int, 0), ('downs', int, 0), ('num_comments', int, 0),\n",
    "    ('score', int, 0), ('upvote_ratio', float, 0.0)\n",
    "]:\n",
    "    if c not in reddit:\n",
    "        reddit[c] = default\n",
    "    reddit[c] = reddit[c].fillna(default).astype(dtp)\n",
    "\n",
    "# Ticker column\n",
    "if 'ticker' not in reddit:\n",
    "    if 'Ticker_id' in reddit_df.columns:\n",
    "        reddit['ticker'] = reddit_df['Ticker_id'].astype(str)\n",
    "    else:\n",
    "        raise ValueError('ticker/Ticker_id column required in reddit_df')\n",
    "\n",
    "# Yahoo Finance CSV has multi-level headers (field, ticker). Re-parse and pivot to long format.\n",
    "# Example fields: Close, High, Low, Open, Volume\n",
    "yf_path = 'year_yf_data.csv'\n",
    "try:\n",
    "    yf_multi = pd.read_csv(yf_path, header=[0, 1], index_col=0, skiprows=[2])\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f'Failed to parse {yf_path} with multi-level headers: {e}')\n",
    "\n",
    "# Ensure index is datetime-like (dates in the first column)\n",
    "yf_multi.index.name = 'date'\n",
    "try:\n",
    "    yf_multi.index = pd.to_datetime(yf_multi.index, errors='coerce')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Helper to convert a category to long format\n",
    "def _to_long(cat: str) -> pd.DataFrame:\n",
    "    if cat not in yf_multi.columns.get_level_values(0):\n",
    "        return pd.DataFrame(columns=['date', 'ticker', cat])\n",
    "    dfc = yf_multi[cat]\n",
    "    long_df = dfc.stack().reset_index()\n",
    "    long_df.columns = ['date', 'ticker', cat]\n",
    "    return long_df\n",
    "\n",
    "close_long = _to_long('Close')\n",
    "vol_long = _to_long('Volume')\n",
    "\n",
    "# Merge desired fields\n",
    "if close_long.empty and vol_long.empty:\n",
    "    yf = pd.DataFrame(columns=['date', 'ticker', 'Close', 'Volume'])\n",
    "else:\n",
    "    yf = close_long.merge(vol_long, on=['date', 'ticker'], how='outer')\n",
    "\n",
    "# Finalize types\n",
    "yf['date'] = pd.to_datetime(yf['date'], errors='coerce').dt.date\n",
    "yf['ticker'] = yf['ticker'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9445d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding posts (FinBERT)...\n",
      "Built 0 sequences across 12 tickers\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# FinBERT embeddings + sentiment per post\n",
    "print('Embedding posts (FinBERT)...')\n",
    "fin_tok, fin_model = load_finbert(device)\n",
    "embeds, sent = finbert_embed_and_sentiment(reddit['text'].tolist(), fin_tok, fin_model, device)\n",
    "reddit['embed'] = [e.cpu().numpy() if isinstance(e, torch.Tensor) else e for e in embeds]\n",
    "reddit['sentiment'] = sent.cpu().numpy()\n",
    "\n",
    "# Daily aggregation per ticker-date\n",
    "reddit['date'] = reddit['created_utc'].dt.tz_convert('US/Eastern').dt.date\n",
    "agg_rows = []\n",
    "for (tic, dt), g in reddit.groupby(['ticker','date'], sort=True):\n",
    "    arr = np.stack(g['embed'].values, axis=0)\n",
    "    ups = g['ups'].to_numpy()\n",
    "    w = ups.astype(float)\n",
    "    w = w / max(w.sum(), 1.0)\n",
    "    day_embed = (arr * w[:, None]).sum(axis=0).astype(np.float32)\n",
    "    row = {\n",
    "        'ticker': tic,\n",
    "        'date': dt,\n",
    "        'embed': day_embed,\n",
    "        'sentiment_mean': float(g['sentiment'].mean()),\n",
    "        'sentiment_weighted': float((g['sentiment'] * (ups / max(ups.sum(), 1))).sum()),\n",
    "        'total_upvotes': int(ups.sum()),\n",
    "        'avg_upvotes': float(ups.mean()),\n",
    "        'comment_engagement': float((g['num_comments'] / np.maximum(ups + g['downs'].to_numpy(), 1)).mean()),\n",
    "    }\n",
    "    agg_rows.append(row)\n",
    "agg = pd.DataFrame(agg_rows)\n",
    "\n",
    "# Merge OHLCV\n",
    "merged = agg.merge(yf.rename(columns={'Ticker':'ticker'}), how='left', on=['ticker','date'])\n",
    "\n",
    "# Build 2-day sequences and labels (next-week direction/magnitude)\n",
    "seq_len = 5\n",
    "sequences = []\n",
    "lengths = []\n",
    "labels_dir = []\n",
    "labels_mag = []\n",
    "meta = []\n",
    "\n",
    "for tic, g in merged.sort_values('date').groupby('ticker'):\n",
    "    g = g.reset_index(drop=True)\n",
    "    # construct daily feature vectors\n",
    "    def feat_vec(r):\n",
    "        return np.concatenate([\n",
    "            r['embed'],\n",
    "            np.array([\n",
    "                r.get('Close', np.nan),\n",
    "                r.get('Volume', np.nan),\n",
    "                r.get('sentiment_mean', 0.0),\n",
    "                r.get('sentiment_weighted', 0.0),\n",
    "                r.get('total_upvotes', 0.0),\n",
    "                r.get('avg_upvotes', 0.0),\n",
    "                r.get('comment_engagement', 0.0),\n",
    "            ], dtype=np.float32)\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "    feats = [feat_vec(r) for _, r in g.iterrows()]\n",
    "    closes = g['Close'].to_numpy()\n",
    "\n",
    "    if len(feats) < seq_len + 5:\n",
    "        # need at least 5 future days to compute next-week movement\n",
    "        continue\n",
    "\n",
    "    # Use the last window we can build with following week\n",
    "    for i in range(0, len(feats) - seq_len - 5 + 1):\n",
    "        window = np.stack(feats[i:i+seq_len], axis=0)\n",
    "        # Next week direction/magnitude from close prices\n",
    "        c0 = closes[i+seq_len-1]\n",
    "        c1 = closes[i+seq_len+2-1]\n",
    "        if np.isnan(c0) or np.isnan(c1):\n",
    "            continue\n",
    "        ret = (c1 - c0) / max(abs(c0), 1e-6)\n",
    "        direction = 1.0 if ret > 0 else 0.0\n",
    "        magnitude = float(abs(ret))\n",
    "\n",
    "        sequences.append(torch.tensor(window, dtype=torch.float32))\n",
    "        lengths.append(seq_len)\n",
    "        labels_dir.append(direction)\n",
    "        labels_mag.append(magnitude)\n",
    "        meta.append({'ticker': tic, 'start_index': i})\n",
    "\n",
    "print(f'Built {len(sequences)} sequences across {merged[\"ticker\"].nunique()} tickers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b0bc697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sequences built. Check your input coverage and date ranges.\n"
     ]
    }
   ],
   "source": [
    "from dataset import WeeklySentimentDataset, collate_weekly\n",
    "from model import LSTMClassifier\n",
    "\n",
    "# Build dataset\n",
    "if len(sequences) == 0:\n",
    "    print('No sequences built. Check your input coverage and date ranges.')\n",
    "else:\n",
    "    ds = WeeklySentimentDataset(\n",
    "        sequences=sequences,\n",
    "        lengths=lengths,\n",
    "        direction=torch.tensor(labels_dir),\n",
    "        magnitude=torch.tensor(labels_mag),\n",
    "        metadata=meta,\n",
    "    )\n",
    "    batch = collate_weekly([ds[i] for i in range(min(8, len(ds)))], pad_to=5)\n",
    "\n",
    "    input_dim = sequences[0].shape[1]\n",
    "    model = LSTMClassifier(input_dim=input_dim).to(device)\n",
    "    out = model(batch['x'].to(device), batch['lengths'].to(device))\n",
    "    {k: tuple(v.shape) for k, v in out.items() if isinstance(v, torch.Tensor)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
