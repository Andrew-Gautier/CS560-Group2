{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11cbac8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!usr/bin/env python3\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import praw\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f19d17",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "root = Path.cwd()\n",
    "compressed_data_root = root / 'posts'\n",
    "reddit_csv_path = root.parent / '10y_reddit_data.csv'\n",
    "stock_csv_path = root.parent / '10y_stock_data.csv'\n",
    "\n",
    "subreddits = [\"stocks\", \"StockMarket\", \"investing\", \"wallstreetbets\", \"options\", \"trading\"]\n",
    "\n",
    "stock_dict = {\n",
    "    'nvidia': ['nvda', 'nvidia'],\n",
    "    'tesla': ['tsla', 'tesla'],\n",
    "    'apple': ['aapl', 'apple'],\n",
    "    'amazon': ['amzn', 'amazon'],\n",
    "    'microsoft': ['msft', 'microsoft'],\n",
    "    'google': ['googl', 'google', 'alphabet']\n",
    "}\n",
    "\n",
    "tickers = ['nvda', 'tsla', 'aapl', 'amzn', 'msft', 'googl']\n",
    "\n",
    "praw_api = praw.Reddit(\n",
    "            client_id=\"5uFqCBUPadVnIxKHG0hnhw\",\n",
    "            client_secret=\"LDFyai0bjEAkEQqo5joU7PjSjtq2eQ\",\n",
    "            user_agent=\"TeslaScraper:v1.0 (by u/RecognitionSame5433)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3056ea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class RedditData:\n",
    "    def __init__(self, years, subreddits=subreddits, stock_dict=stock_dict, praw_api=praw_api):\n",
    "        self.subreddits = subreddits\n",
    "        self.stock_dict = stock_dict\n",
    "        self.api = praw_api\n",
    "        self.years = years\n",
    "        self.df = None\n",
    "\n",
    "    def search_subreddit(self, subreddit_name:str, query:str):\n",
    "        subreddit = self.api.subreddit(subreddit_name)\n",
    "        gen = subreddit.search(query, sort='new', time_filter='year', limit=1000)\n",
    "        rows = []\n",
    "        for s in tqdm(gen, desc=\"pulling submissions\"):\n",
    "            rows.append({\n",
    "                    \"id\": s.id,\n",
    "                    \"created_utc\": pd.to_datetime(s.created_utc, unit=\"s\"),\n",
    "                    \"ticker\" : query,\n",
    "                    \"subreddit\": str(s.subreddit),\n",
    "                    \"author\": str(s.author) if s.author else None,\n",
    "                    \"title\": s.title,\n",
    "                    \"selftext\": s.selftext,\n",
    "                    \"score\": s.score,\n",
    "                    \"upvote_ratio\": s.upvote_ratio,\n",
    "                    \"num_comments\": s.num_comments,\n",
    "                })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    # Get year of data for all subreddits and queries\n",
    "    def load_api_data(self):\n",
    "        df_list = []\n",
    "        for stock, query_list in self.stock_dict.items():\n",
    "            for query in query_list:\n",
    "                for sub in self.subreddits:\n",
    "                    df = self.search_subreddit(sub, query)\n",
    "                    df['query'] = query\n",
    "                    df['stock'] = stock\n",
    "                    df['subreddit'] = sub\n",
    "                    df_list.append(df)\n",
    "        new_df = pd.concat(df_list)\n",
    "        self.add_data(new_df)\n",
    "\n",
    "    # Parses all data for \n",
    "    def load_compressed_ndjson(self, root: Path):\n",
    "        df_list = []\n",
    "        for file_path in root.rglob('*.zst'):\n",
    "            df_list.append(pd.read_json(\n",
    "                file_path,\n",
    "                lines=True,\n",
    "                compression='zstd'\n",
    "            ))\n",
    "        new_df = pd.concat(df_list)\n",
    "        self.add_data(new_df)\n",
    "\n",
    "    def add_data(self, df):\n",
    "        df = RedditData.drop_duplicates(df)\n",
    "        if self.df is None:\n",
    "            self.df = df\n",
    "        else:\n",
    "            new_df = pd.concat([self.df, df])\n",
    "            overlap = len(new_df.duplicated())\n",
    "            if overlap == 0:\n",
    "                raise ValueError('Time ranges do not overlap')\n",
    "            print(f'Dataframes overlap of {overlap} rows')\n",
    "            self.df = RedditData.drop_duplicates(new_df)\n",
    "\n",
    "    @staticmethod\n",
    "    def restrict_time(df, years):\n",
    "        if df is None:\n",
    "            raise ValueError('No data loaded')\n",
    "        cutoff = pd.Timestamp.today() - pd.DateOffset(years=years)\n",
    "        df = df[df. >= cutoff]\n",
    "\n",
    "    # Remove duplicates for (id, stock) and enforce timeframe\n",
    "    @staticmethod\n",
    "    def drop_duplicates(df):\n",
    "        if df is None:\n",
    "            raise ValueError('Dataframe is None')\n",
    "        df.drop_duplicates(subset=['id', 'stock'], inplace=True)\n",
    "\n",
    "\n",
    "class StockData:\n",
    "    def __init__(self, tickers, period='10y', interval='1d'):\n",
    "        self.tickers = tickers\n",
    "        df = yf.download(tickers, period=period, interval=interval, auto_adjust=True)\n",
    "        if df is None:\n",
    "            raise ValueError('Data failed to download')\n",
    "        self.df = df\n",
    "\n",
    "    # Forward fill OHLC and zero Volume\n",
    "    def impute_off_days(self):\n",
    "        full_idx = pd.date_range(self.df.index.min(), self.df.index.max(), freq='D')\n",
    "        self.df.reindex(full_idx)\n",
    "        self.df['Volume'] = self.df['Volume'].fillna(0)\n",
    "        price_cols = ['Open', 'High', 'Low', 'Close']\n",
    "        self.df[price_cols] = self.df[price_cols].ffill()\n",
    "\n",
    "    # Create percent change columns for prices\n",
    "    def create_pct_columns(self):\n",
    "        price_cols = ['Open', 'High', 'Low', 'Close']\n",
    "        new_cols = [x+'_pct' for x in price_cols]\n",
    "        self.df[new_cols] = self.df[price_cols].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d327c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create reddit csv\n",
    "    reddit_data = RedditData(years=10)\n",
    "    reddit_data.load_compressed_ndjson(compressed_data_root)\n",
    "    reddit_data.load_api_data()\n",
    "    assert isinstance(reddit_data.df, pd.DataFrame)\n",
    "    reddit_data.df.to_csv(reddit_csv_path, encoding='utf-8', index=True, header=True, sep=',')\n",
    "\n",
    "    # Create stock csv\n",
    "    stock_data = StockData(tickers, period='10y')\n",
    "    stock_data.impute_off_days()\n",
    "    stock_data.create_pct_columns()\n",
    "    stock_data.df.to_csv(stock_csv_path, encoding='utf-8', index=True, header=True, sep=',')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
