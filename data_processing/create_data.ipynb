{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e11cbac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!usr/bin/env python3\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import praw\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e69cc44",
   "metadata": {},
   "source": [
    "# Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6f19d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path.cwd()\n",
    "compressed_data_root = root / 'posts'\n",
    "reddit_csv_path = root / '10y_reddit_data.csv.zst'\n",
    "stock_csv_path = root / '10y_stock_data.csv.zst'\n",
    "\n",
    "years=10\n",
    "\n",
    "subreddits = [\"stocks\", \"StockMarket\", \"investing\", \"wallstreetbets\", \"options\", \"trading\"]\n",
    "\n",
    "stock_dict = {\n",
    "    'nvidia': ['nvda', 'nvidia'],\n",
    "    'tesla': ['tsla', 'tesla'],\n",
    "    'apple': ['aapl', 'apple'],\n",
    "    'amazon': ['amzn', 'amazon'],\n",
    "    'microsoft': ['msft', 'microsoft'],\n",
    "    'google': ['googl', 'google', 'alphabet']\n",
    "}\n",
    "\n",
    "tickers = ['nvda', 'tsla', 'aapl', 'amzn', 'msft', 'googl']\n",
    "\n",
    "praw_api = praw.Reddit(\n",
    "            client_id=\"5uFqCBUPadVnIxKHG0hnhw\",\n",
    "            client_secret=\"LDFyai0bjEAkEQqo5joU7PjSjtq2eQ\",\n",
    "            user_agent=\"TeslaScraper:v1.0 (by u/RecognitionSame5433)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb1bf4",
   "metadata": {},
   "source": [
    "# Reddit data fields\n",
    "- INDEX: Day normalized timestamp\n",
    "- id\n",
    "- created_utc\n",
    "- author\n",
    "- title\n",
    "- selftext\n",
    "- score\n",
    "- num_comments\n",
    "- query (only for api data)\n",
    "- stock\n",
    "- subreddit\n",
    "- source (api or archive)\n",
    "\n",
    "# Stock data fields per ticker\n",
    "- INDEX: Day normalized timestamp\n",
    "- Open\n",
    "- High\n",
    "- Low\n",
    "- Close\n",
    "- Volume\n",
    "- Open_pct\n",
    "- High_pct\n",
    "- Low_pct\n",
    "- Close_pct\n",
    "- *Revenue\n",
    "- *Earnings\n",
    "- *Revenue_pct\n",
    "- *Earnings_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3056ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditData:\n",
    "    def __init__(self, years, subreddits, stock_dict, praw_api):\n",
    "        self.subreddits = subreddits\n",
    "        self.stock_dict = stock_dict\n",
    "        self.api = praw_api\n",
    "        self.timedelta = pd.Timedelta(days=365 * years)\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "    def search_subreddit(self, subreddit_name:str, query:str):\n",
    "        subreddit = self.api.subreddit(subreddit_name)\n",
    "        gen = subreddit.search(query, sort='new', time_filter='year', limit=1000)\n",
    "        rows = []\n",
    "        for s in gen:\n",
    "            rows.append({\n",
    "                    \"id\": s.id,\n",
    "                    \"created_utc\" : s.created_utc,\n",
    "                    \"author\": str(s.author) if s.author else None,\n",
    "                    \"title\": s.title,\n",
    "                    \"selftext\": s.selftext,\n",
    "                    \"score\": s.score,\n",
    "                    \"num_comments\": s.num_comments,\n",
    "                })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    # Get year of data for all subreddits and queries\n",
    "    def load_api_data(self):\n",
    "        df_list = []\n",
    "        all_query_pairs = [(stock, query)\n",
    "                           for stock, query_list in self.stock_dict.items()\n",
    "                           for query in query_list\n",
    "                           ]\n",
    "        for stock, query in tqdm(all_query_pairs, desc='Queries', position=0):\n",
    "            for sub in tqdm(self.subreddits,\n",
    "                            desc='Subreddits',\n",
    "                            leave=False,\n",
    "                            position=1):\n",
    "                df = self.search_subreddit(sub, query)\n",
    "                df['timestamp'] = (\n",
    "                    pd.to_datetime(df['created_utc'], unit='s', utc=True)\n",
    "                    .dt.normalize()\n",
    "                )\n",
    "                df = df.set_index('timestamp')\n",
    "                df['query'] = query\n",
    "                df['stock'] = stock\n",
    "                df['subreddit'] = sub\n",
    "                df_list.append(df)\n",
    "        nonempty_dfs = [df for df in df_list if not df.empty]\n",
    "        print(f'{len(nonempty_dfs)} out of {len(df_list)} queries had hits')\n",
    "        new_df = pd.concat(df_list)\n",
    "        new_df['query'] = ''\n",
    "        self.add_data(new_df, 'api')\n",
    "        print('API data loaded')\n",
    "\n",
    "    # Parses all compressed data root\n",
    "    def load_compressed_ndjson(self, root: Path):\n",
    "        df_list = []\n",
    "        for file_path in root.rglob('*.zst'):\n",
    "            print(f'Extracting {file_path.name}')\n",
    "            subreddit = file_path.stem.removesuffix('_submissions')\n",
    "            chunk_iter = pd.read_json(\n",
    "                file_path,\n",
    "                lines=True,\n",
    "                compression='zstd',\n",
    "                chunksize=2**16\n",
    "            )\n",
    "            df_chunks = []\n",
    "            # Process and append each chunk\n",
    "            for df_chunk in tqdm(chunk_iter,\n",
    "                                desc='File chunks processed',\n",
    "                                leave=False):\n",
    "                df_chunk['timestamp'] = (\n",
    "                    pd.to_datetime(df_chunk['created_utc'], unit='s', utc=True)\n",
    "                    .dt.normalize()\n",
    "                )\n",
    "                df_chunk = df_chunk.set_index('timestamp')\n",
    "                df_chunk = self.restrict_time_for(df_chunk)\n",
    "                df_chunk = self.restrict_columns_for(df_chunk)\n",
    "                df_chunk = self.keyword_filter_for(df_chunk)\n",
    "                df_chunks.append(df_chunk)\n",
    "            if not df_chunks:\n",
    "                raise RuntimeError('File yielded no rows')\n",
    "            df = pd.concat(df_chunks)\n",
    "            df = self.drop_duplicates_for(df)\n",
    "            df['subreddit'] = subreddit\n",
    "            df_list.append(df)\n",
    "        if not df_list:\n",
    "            raise RuntimeError('Failed to load any data from files')\n",
    "        new_df = pd.concat(df_list)\n",
    "        self.add_data(new_df, 'archive')\n",
    "        print('Compressed data loaded')\n",
    "    \n",
    "    # Remove duplicates and check for overlap when necessary\n",
    "    def add_data(self, df, source):\n",
    "        df = self.drop_duplicates_for(df)\n",
    "        df = self.drop_invalid_posts_for(df)\n",
    "        df['source'] = source\n",
    "        if self.df.empty:\n",
    "            self.df = df\n",
    "        else:\n",
    "            new_df = pd.concat([self.df, df])\n",
    "            overlap = new_df.duplicated(subset=['id', 'stock']).sum()\n",
    "            if overlap == 0:\n",
    "                raise ValueError('Time ranges do not overlap')\n",
    "            print(f'Dataframe overlap of {overlap} rows')\n",
    "            self.df = self.drop_duplicates_for(new_df).sort_index()\n",
    "\n",
    "    def keyword_filter_for(self, df):\n",
    "        keyword_cols = ['title', 'selftext']\n",
    "        text_df = df[keyword_cols]\n",
    "        df_list = []\n",
    "        for stock, query_list in self.stock_dict.items():\n",
    "            pattern = '|'.join(re.escape(q) for q in query_list)\n",
    "            mask = (\n",
    "                text_df\n",
    "                .apply(lambda col: col.astype(str).str.contains(pattern, case=False, na=False))\n",
    "                .any(axis=1)\n",
    "            )\n",
    "            df_part = df[mask].copy()\n",
    "            df_part['stock'] = stock\n",
    "            df_list.append(df_part)\n",
    "        if df_list:\n",
    "            new_df = pd.concat(df_list)\n",
    "            new_df = self.drop_duplicates_for(new_df)\n",
    "        else:\n",
    "            new_df = df.iloc[0:0].copy()\n",
    "        return new_df\n",
    "    \n",
    "    def drop_invalid_posts_for(self, df):\n",
    "        keywords = ['[removed]', '[deleted]']\n",
    "        pattern = '|'.join([re.escape(s) for s in keywords])\n",
    "        mask = (\n",
    "            df.selftext.str.contains(pattern) |\n",
    "            df.title.str.contains(pattern)\n",
    "        )\n",
    "        new_df = df[~mask]\n",
    "        return new_df\n",
    "            \n",
    "    def restrict_columns_for(self, df):\n",
    "        reddit_columns = ['id', 'created_utc', 'author', 'title', 'selftext', 'score', 'num_comments']\n",
    "        new_df = df[reddit_columns].copy()\n",
    "        return new_df\n",
    "\n",
    "    def restrict_time_for(self, df):\n",
    "        if not str(df.index.dtype) == \"datetime64[ns, UTC]\":\n",
    "            raise RuntimeError(\n",
    "                f'Invalid index type: {str(df.index.dtype)}')\n",
    "        start_timestamp = pd.Timestamp.utcnow() - self.timedelta\n",
    "        new_df = df.sort_index().loc[start_timestamp:]\n",
    "        return new_df\n",
    "\n",
    "    # Remove duplicates for (id, stock) and enforce timeframe\n",
    "    def drop_duplicates_for(self, df):\n",
    "        new_df = df.drop_duplicates(subset=['id', 'stock'], inplace=False)\n",
    "        return new_df\n",
    "\n",
    "\n",
    "class StockData:\n",
    "    def __init__(self, tickers, years, interval='1d'):\n",
    "        self.tickers = tickers\n",
    "        df = yf.download(tickers, period=f'{years}y', interval=interval, auto_adjust=True)\n",
    "        if df is None:\n",
    "            raise ValueError('Data failed to download')\n",
    "        assert isinstance(df.index, pd.DatetimeIndex)\n",
    "        df.index = df.index.tz_localize('UTC')\n",
    "        self.df = df\n",
    "\n",
    "    # Forward fill OHLC and zero Volume\n",
    "    def impute_off_days(self):\n",
    "        full_idx = pd.date_range(self.df.index.min(),\n",
    "                                 self.df.index.max(),\n",
    "                                 freq='D',\n",
    "                                 tz='UTC')\n",
    "        self.df = self.df.reindex(full_idx)\n",
    "        self.df['Volume'] = self.df['Volume'].fillna(0)\n",
    "        self.df = self.df.ffill()\n",
    "\n",
    "    # Create percent change columns for prices\n",
    "    def create_pct_columns(self):\n",
    "        price_fields = ['Open', 'High', 'Low', 'Close']\n",
    "        price_df = self.df[price_fields]\n",
    "        pct_df = price_df.pct_change()\n",
    "        l0_fields = pct_df.columns.get_level_values(0)\n",
    "        l1_fields = pct_df.columns.get_level_values(1)\n",
    "        l0_fields = [s+'_pct' for s in l0_fields]\n",
    "        pct_df.columns = pd.MultiIndex.from_arrays(\n",
    "            [l0_fields, l1_fields], names=pct_df.columns.names)\n",
    "        self.df = pd.concat([self.df, pct_df], axis=1).sort_index(axis=1)\n",
    "        self.df = self.df.drop(self.df.index.min())\n",
    "\n",
    "\n",
    "def align_data(reddit_data, stock_data):\n",
    "    min_date = max(reddit_data.df.index.min(), stock_data.df.index.min())\n",
    "    max_date = min(reddit_data.df.index.max(), stock_data.df.index.max())\n",
    "    reddit_data.df = reddit_data.df.loc[min_date:max_date]\n",
    "    stock_data.df = stock_data.df.loc[min_date:max_date]\n",
    "\n",
    "def reddit_csv():\n",
    "    return pd.read_csv('10y_reddit_data.csv.zst',\n",
    "                              encoding='utf-8',\n",
    "                              parse_dates=True)\n",
    "\n",
    "def stock_csv():\n",
    "    return pd.read_csv('10y_stock_data.csv.zst',\n",
    "                              header=[0, 1],\n",
    "                              index_col=0,\n",
    "                              parse_dates=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d11246",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = RedditData(years, subreddits, stock_dict, praw_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82944a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data.load_compressed_ndjson(compressed_data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b30709",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data.load_api_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ad352",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((reddit_data.df['source'] == 'api').sum())\n",
    "print((reddit_data.df['source'] == 'archive').sum())\n",
    "api_df = reddit_data.df[reddit_data.df['source'] == 'api']\n",
    "arc_df = reddit_data.df[reddit_data.df['source'] == 'archive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80a600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reddit_data.df['id'].nunique())\n",
    "print(api_df['id'].nunique())\n",
    "print(arc_df['id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d7c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = StockData(tickers, years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b8eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.impute_off_days()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2addcbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.create_pct_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c97920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "align_data(reddit_data, stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee2b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de5b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.df.to_csv(stock_csv_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297de630",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data.df.to_csv(reddit_csv_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f90ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33572/3095459165.py:197: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv('10y_reddit_data.csv.zst',\n"
     ]
    }
   ],
   "source": [
    "rdf = reddit_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f550e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = stock_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "394af438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>stock</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>source</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-05 00:00:00+00:00</td>\n",
       "      <td>3vj1vg</td>\n",
       "      <td>1.449312e+09</td>\n",
       "      <td>formahyi</td>\n",
       "      <td>short AMZN straddle</td>\n",
       "      <td>595/740 expiring in 40 days  \\n\\nprimum collec...</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>amazon</td>\n",
       "      <td>options</td>\n",
       "      <td>archive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-05 00:00:00+00:00</td>\n",
       "      <td>3vk6qf</td>\n",
       "      <td>1.449337e+09</td>\n",
       "      <td>Johnny_Fratto</td>\n",
       "      <td>ITT: We make financial predictions regarding t...</td>\n",
       "      <td>Apple will hit $140/share\\n\\nTesla will go dow...</td>\n",
       "      <td>36</td>\n",
       "      <td>73</td>\n",
       "      <td>tesla</td>\n",
       "      <td>investing</td>\n",
       "      <td>archive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-05 00:00:00+00:00</td>\n",
       "      <td>3vk6qf</td>\n",
       "      <td>1.449337e+09</td>\n",
       "      <td>Johnny_Fratto</td>\n",
       "      <td>ITT: We make financial predictions regarding t...</td>\n",
       "      <td>Apple will hit $140/share\\n\\nTesla will go dow...</td>\n",
       "      <td>36</td>\n",
       "      <td>73</td>\n",
       "      <td>apple</td>\n",
       "      <td>investing</td>\n",
       "      <td>archive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-06 00:00:00+00:00</td>\n",
       "      <td>3vpco3</td>\n",
       "      <td>1.449436e+09</td>\n",
       "      <td>canoned</td>\n",
       "      <td>Fellow investors, what are you thoughts on inv...</td>\n",
       "      <td>FB and Oculus? Google? Sony? HTC/Samsung? Othe...</td>\n",
       "      <td>74</td>\n",
       "      <td>93</td>\n",
       "      <td>google</td>\n",
       "      <td>investing</td>\n",
       "      <td>archive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-12-06 00:00:00+00:00</td>\n",
       "      <td>3vm4id</td>\n",
       "      <td>1.449369e+09</td>\n",
       "      <td>j3wbakka</td>\n",
       "      <td>YACHTpportunity in NTES</td>\n",
       "      <td>Alright you sexy fuckers, let me hit you with ...</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>microsoft</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>archive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp      id   created_utc         author  \\\n",
       "0  2015-12-05 00:00:00+00:00  3vj1vg  1.449312e+09       formahyi   \n",
       "1  2015-12-05 00:00:00+00:00  3vk6qf  1.449337e+09  Johnny_Fratto   \n",
       "2  2015-12-05 00:00:00+00:00  3vk6qf  1.449337e+09  Johnny_Fratto   \n",
       "3  2015-12-06 00:00:00+00:00  3vpco3  1.449436e+09        canoned   \n",
       "4  2015-12-06 00:00:00+00:00  3vm4id  1.449369e+09       j3wbakka   \n",
       "\n",
       "                                               title  \\\n",
       "0                                short AMZN straddle   \n",
       "1  ITT: We make financial predictions regarding t...   \n",
       "2  ITT: We make financial predictions regarding t...   \n",
       "3  Fellow investors, what are you thoughts on inv...   \n",
       "4                            YACHTpportunity in NTES   \n",
       "\n",
       "                                            selftext  score  num_comments  \\\n",
       "0  595/740 expiring in 40 days  \\n\\nprimum collec...      3            27   \n",
       "1  Apple will hit $140/share\\n\\nTesla will go dow...     36            73   \n",
       "2  Apple will hit $140/share\\n\\nTesla will go dow...     36            73   \n",
       "3  FB and Oculus? Google? Sony? HTC/Samsung? Othe...     74            93   \n",
       "4  Alright you sexy fuckers, let me hit you with ...     12             9   \n",
       "\n",
       "       stock       subreddit   source query  \n",
       "0     amazon         options  archive   NaN  \n",
       "1      tesla       investing  archive   NaN  \n",
       "2      apple       investing  archive   NaN  \n",
       "3     google       investing  archive   NaN  \n",
       "4  microsoft  wallstreetbets  archive   NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "372f6e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th colspan=\"6\" halign=\"left\">Close</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Close_pct</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Open_pct</th>\n",
       "      <th colspan=\"6\" halign=\"left\">Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>...</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>TSLA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-12-05 00:00:00+00:00</th>\n",
       "      <td>26.837568</td>\n",
       "      <td>33.632000</td>\n",
       "      <td>38.693630</td>\n",
       "      <td>48.955368</td>\n",
       "      <td>0.823295</td>\n",
       "      <td>15.358667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-06 00:00:00+00:00</th>\n",
       "      <td>26.837568</td>\n",
       "      <td>33.632000</td>\n",
       "      <td>38.693630</td>\n",
       "      <td>48.955368</td>\n",
       "      <td>0.823295</td>\n",
       "      <td>15.358667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-07 00:00:00+00:00</th>\n",
       "      <td>26.668463</td>\n",
       "      <td>33.491501</td>\n",
       "      <td>38.384758</td>\n",
       "      <td>48.867832</td>\n",
       "      <td>0.807683</td>\n",
       "      <td>15.408667</td>\n",
       "      <td>-0.006301</td>\n",
       "      <td>-0.004178</td>\n",
       "      <td>-0.007983</td>\n",
       "      <td>-0.001788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013446</td>\n",
       "      <td>0.030858</td>\n",
       "      <td>0.040395</td>\n",
       "      <td>-0.020477</td>\n",
       "      <td>128336800.0</td>\n",
       "      <td>74656000.0</td>\n",
       "      <td>36626000.0</td>\n",
       "      <td>30709800.0</td>\n",
       "      <td>333588000.0</td>\n",
       "      <td>47163000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-08 00:00:00+00:00</th>\n",
       "      <td>26.657188</td>\n",
       "      <td>33.866501</td>\n",
       "      <td>38.491520</td>\n",
       "      <td>48.850327</td>\n",
       "      <td>0.818661</td>\n",
       "      <td>15.114667</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>0.011197</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>-0.000358</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019947</td>\n",
       "      <td>-0.005736</td>\n",
       "      <td>-0.030824</td>\n",
       "      <td>-0.000791</td>\n",
       "      <td>137238000.0</td>\n",
       "      <td>73038000.0</td>\n",
       "      <td>31082000.0</td>\n",
       "      <td>32878000.0</td>\n",
       "      <td>314488000.0</td>\n",
       "      <td>40314000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-09 00:00:00+00:00</th>\n",
       "      <td>26.068716</td>\n",
       "      <td>33.239498</td>\n",
       "      <td>37.866337</td>\n",
       "      <td>48.141064</td>\n",
       "      <td>0.797926</td>\n",
       "      <td>14.968000</td>\n",
       "      <td>-0.022076</td>\n",
       "      <td>-0.018514</td>\n",
       "      <td>-0.016242</td>\n",
       "      <td>-0.014519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008633</td>\n",
       "      <td>-0.001803</td>\n",
       "      <td>0.025994</td>\n",
       "      <td>-0.003604</td>\n",
       "      <td>185445600.0</td>\n",
       "      <td>103164000.0</td>\n",
       "      <td>46550000.0</td>\n",
       "      <td>36373200.0</td>\n",
       "      <td>372676000.0</td>\n",
       "      <td>45867000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Price                          Close                                   \\\n",
       "Ticker                          AAPL       AMZN      GOOGL       MSFT   \n",
       "2015-12-05 00:00:00+00:00  26.837568  33.632000  38.693630  48.955368   \n",
       "2015-12-06 00:00:00+00:00  26.837568  33.632000  38.693630  48.955368   \n",
       "2015-12-07 00:00:00+00:00  26.668463  33.491501  38.384758  48.867832   \n",
       "2015-12-08 00:00:00+00:00  26.657188  33.866501  38.491520  48.850327   \n",
       "2015-12-09 00:00:00+00:00  26.068716  33.239498  37.866337  48.141064   \n",
       "\n",
       "Price                                          Close_pct                      \\\n",
       "Ticker                         NVDA       TSLA      AAPL      AMZN     GOOGL   \n",
       "2015-12-05 00:00:00+00:00  0.823295  15.358667  0.000000  0.000000  0.000000   \n",
       "2015-12-06 00:00:00+00:00  0.823295  15.358667  0.000000  0.000000  0.000000   \n",
       "2015-12-07 00:00:00+00:00  0.807683  15.408667 -0.006301 -0.004178 -0.007983   \n",
       "2015-12-08 00:00:00+00:00  0.818661  15.114667 -0.000423  0.011197  0.002781   \n",
       "2015-12-09 00:00:00+00:00  0.797926  14.968000 -0.022076 -0.018514 -0.016242   \n",
       "\n",
       "Price                                ...  Open_pct                      \\\n",
       "Ticker                         MSFT  ...     GOOGL      MSFT      NVDA   \n",
       "2015-12-05 00:00:00+00:00  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2015-12-06 00:00:00+00:00  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2015-12-07 00:00:00+00:00 -0.001788  ...  0.013446  0.030858  0.040395   \n",
       "2015-12-08 00:00:00+00:00 -0.000358  ... -0.019947 -0.005736 -0.030824   \n",
       "2015-12-09 00:00:00+00:00 -0.014519  ...  0.008633 -0.001803  0.025994   \n",
       "\n",
       "Price                                     Volume                           \\\n",
       "Ticker                         TSLA         AAPL         AMZN       GOOGL   \n",
       "2015-12-05 00:00:00+00:00  0.000000          0.0          0.0         0.0   \n",
       "2015-12-06 00:00:00+00:00  0.000000          0.0          0.0         0.0   \n",
       "2015-12-07 00:00:00+00:00 -0.020477  128336800.0   74656000.0  36626000.0   \n",
       "2015-12-08 00:00:00+00:00 -0.000791  137238000.0   73038000.0  31082000.0   \n",
       "2015-12-09 00:00:00+00:00 -0.003604  185445600.0  103164000.0  46550000.0   \n",
       "\n",
       "Price                                                           \n",
       "Ticker                           MSFT         NVDA        TSLA  \n",
       "2015-12-05 00:00:00+00:00         0.0          0.0         0.0  \n",
       "2015-12-06 00:00:00+00:00         0.0          0.0         0.0  \n",
       "2015-12-07 00:00:00+00:00  30709800.0  333588000.0  47163000.0  \n",
       "2015-12-08 00:00:00+00:00  32878000.0  314488000.0  40314000.0  \n",
       "2015-12-09 00:00:00+00:00  36373200.0  372676000.0  45867000.0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a763760",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
