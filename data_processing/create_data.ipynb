{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e11cbac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!usr/bin/env python3\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import praw\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e69cc44",
   "metadata": {},
   "source": [
    "# Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6f19d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path.cwd()\n",
    "compressed_data_root = root / 'posts'\n",
    "reddit_csv_path = root.parent / '10y_reddit_data.csv'\n",
    "stock_csv_path = root.parent / '10y_stock_data.csv'\n",
    "\n",
    "years=10\n",
    "\n",
    "subreddits = [\"stocks\", \"StockMarket\", \"investing\", \"wallstreetbets\", \"options\", \"trading\"]\n",
    "\n",
    "stock_dict = {\n",
    "    'nvidia': ['nvda', 'nvidia'],\n",
    "    'tesla': ['tsla', 'tesla'],\n",
    "    'apple': ['aapl', 'apple'],\n",
    "    'amazon': ['amzn', 'amazon'],\n",
    "    'microsoft': ['msft', 'microsoft'],\n",
    "    'google': ['googl', 'google', 'alphabet']\n",
    "}\n",
    "\n",
    "tickers = ['nvda', 'tsla', 'aapl', 'amzn', 'msft', 'googl']\n",
    "\n",
    "praw_api = praw.Reddit(\n",
    "            client_id=\"5uFqCBUPadVnIxKHG0hnhw\",\n",
    "            client_secret=\"LDFyai0bjEAkEQqo5joU7PjSjtq2eQ\",\n",
    "            user_agent=\"TeslaScraper:v1.0 (by u/RecognitionSame5433)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb1bf4",
   "metadata": {},
   "source": [
    "# Reddit data fields\n",
    "- id\n",
    "- created_utc\n",
    "- timestamp\n",
    "- author\n",
    "- title\n",
    "- selftext\n",
    "- score\n",
    "- num_comments\n",
    "- query (only for api data)\n",
    "- stock\n",
    "- subreddit\n",
    "- source (api or archive)\n",
    "\n",
    "# Stock data fields per ticker\n",
    "- Open\n",
    "- High\n",
    "- Low\n",
    "- Close\n",
    "- Volume\n",
    "- Open_pct\n",
    "- High_pct\n",
    "- Low_pct\n",
    "- Close_pct\n",
    "- Revenue\n",
    "- Earnings\n",
    "- Revenue\n",
    "- Earnings\n",
    "- Revenue_pct\n",
    "- Earnings_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3056ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditData:\n",
    "    def __init__(self, years, subreddits, stock_dict, praw_api):\n",
    "        self.subreddits = subreddits\n",
    "        self.stock_dict = stock_dict\n",
    "        self.api = praw_api\n",
    "        self.timedelta = pd.Timedelta(days=365 * years)\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "    def search_subreddit(self, subreddit_name:str, query:str):\n",
    "        subreddit = self.api.subreddit(subreddit_name)\n",
    "        gen = subreddit.search(query, sort='new', time_filter='year', limit=1000)\n",
    "        rows = []\n",
    "        for s in gen:\n",
    "            rows.append({\n",
    "                    \"id\": s.id,\n",
    "                    \"created_utc\" : s.created_utc,\n",
    "                    \"timestamp\": pd.to_datetime(s.created_utc, unit='s', utc=True),\n",
    "                    \"author\": str(s.author) if s.author else None,\n",
    "                    \"title\": s.title,\n",
    "                    \"selftext\": s.selftext,\n",
    "                    \"score\": s.score,\n",
    "                    \"num_comments\": s.num_comments,\n",
    "                })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    # Get year of data for all subreddits and queries\n",
    "    def load_api_data(self):\n",
    "        df_list = []\n",
    "        for stock, query_list in self.stock_dict.items():\n",
    "            for query in tqdm(query_list,\n",
    "                              leave=False,\n",
    "                              total=sum(len(sub) for sub in self.stock_dict.values())):\n",
    "                for sub in tqdm(self.subreddits,\n",
    "                                leave=False,\n",
    "                                total=len(self.subreddits)):\n",
    "                    df = self.search_subreddit(sub, query)\n",
    "                    df['query'] = query\n",
    "                    df['stock'] = stock\n",
    "                    df['subreddit'] = sub\n",
    "                    df_list.append(df)\n",
    "        nonempty_dfs = [df for df in df_list if not df.empty]\n",
    "        print(f'{len(nonempty_dfs)} out of {len(df_list)} queries had hits')\n",
    "        new_df = pd.concat(df_list, ignore_index=True)\n",
    "        self.add_data(new_df, 'api')\n",
    "        print('API data loaded')\n",
    "\n",
    "    # Parses all compressed data root\n",
    "    def load_compressed_ndjson(self, root: Path):\n",
    "        df_list = []\n",
    "        for file_path in root.rglob('*.zst'):\n",
    "            print(f'Extracting {file_path.name}')\n",
    "            subreddit = file_path.stem.removesuffix('_submissions')\n",
    "            chunk_iter = pd.read_json(\n",
    "                file_path,\n",
    "                lines=True,\n",
    "                compression='zstd',\n",
    "                chunksize=2**16\n",
    "            )\n",
    "            df_chunks = []\n",
    "            # Process and append each chunk\n",
    "            for df_chunk in tqdm(chunk_iter, desc='Processing file'):\n",
    "                df_chunk = self.restrict_columns_for(df_chunk)\n",
    "                df_chunk['timestamp'] = pd.to_datetime(df_chunk['created_utc'], unit='s', utc=True)\n",
    "                df_chunk = self.restrict_time_for(df_chunk)\n",
    "                df_chunk = self.keyword_filter_for(df_chunk)\n",
    "                df_chunks.append(df_chunk)\n",
    "            if not df_chunks:\n",
    "                raise RuntimeError('File yielded no rows')\n",
    "            df = pd.concat(df_chunks, ignore_index=True)\n",
    "            df = self.drop_duplicates_for(df)\n",
    "            df['subreddit'] = subreddit\n",
    "            df_list.append(df)\n",
    "        if not df_list:\n",
    "            raise RuntimeError('Failed to load any data from files')\n",
    "        new_df = pd.concat(df_list, ignore_index=True)\n",
    "        self.add_data(new_df, 'archive')\n",
    "        print('Compressed data loaded')\n",
    "    \n",
    "    # Remove duplicates and check for overlap when necessary\n",
    "    def add_data(self, df, source):\n",
    "        df = self.drop_duplicates_for(df)\n",
    "        df['source'] = source\n",
    "        if self.df.empty:\n",
    "            self.df = df\n",
    "        else:\n",
    "            new_df = pd.concat([self.df, df], ignore_index=True)\n",
    "            overlap = new_df.duplicated(subset=['id', 'stock']).sum()\n",
    "            if overlap == 0:\n",
    "                raise ValueError('Time ranges do not overlap')\n",
    "            print(f'Dataframe overlap of {overlap} rows')\n",
    "            self.df = self.drop_duplicates_for(new_df)\n",
    "\n",
    "    def keyword_filter_for(self, df):\n",
    "        keyword_cols = ['title', 'selftext']\n",
    "        text_df = df[keyword_cols]\n",
    "        df_list = []\n",
    "        for stock, query_list in self.stock_dict.items():\n",
    "            pattern = '|'.join(re.escape(q) for q in query_list)\n",
    "            mask = (\n",
    "                text_df\n",
    "                .apply(lambda col: col.astype(str).str.contains(pattern, case=False, na=False))\n",
    "                .any(axis=1)\n",
    "            )\n",
    "            df_part = df[mask].copy()\n",
    "            df_part['stock'] = stock\n",
    "            df_list.append(df_part)\n",
    "        if df_list:\n",
    "            new_df = pd.concat(df_list, ignore_index=True)\n",
    "            new_df = self.drop_duplicates_for(new_df)\n",
    "        else:\n",
    "            new_df = df.iloc[0:0].copy()\n",
    "        return new_df\n",
    "            \n",
    "    def restrict_columns_for(self, df):\n",
    "        reddit_columns = ['id', 'created_utc', 'author', 'title', 'selftext', 'score', 'num_comments']\n",
    "        new_df = df[reddit_columns].copy()\n",
    "        return new_df\n",
    "\n",
    "    def restrict_time_for(self, df):\n",
    "        start_timestamp = pd.Timestamp.utcnow() - self.timedelta\n",
    "        new_df = df[df.timestamp >= start_timestamp].copy()\n",
    "        return new_df\n",
    "\n",
    "    # Remove duplicates for (id, stock) and enforce timeframe\n",
    "    def drop_duplicates_for(self, df):\n",
    "        new_df = df.drop_duplicates(subset=['id', 'stock'], inplace=False)\n",
    "        return new_df\n",
    "\n",
    "\n",
    "class StockData:\n",
    "    def __init__(self, tickers, years, interval='1d'):\n",
    "        self.tickers = tickers\n",
    "        df = yf.download(tickers, period=f'{years}y', interval=interval, auto_adjust=True)\n",
    "        if df is None:\n",
    "            raise ValueError('Data failed to download')\n",
    "        self.df = df\n",
    "\n",
    "    # Forward fill OHLC and zero Volume\n",
    "    def impute_off_days(self):\n",
    "        full_idx = pd.date_range(self.df.index.min(), self.df.index.max(), freq='D')\n",
    "        self.df = self.df.reindex(full_idx)\n",
    "        self.df['Volume'] = self.df['Volume'].fillna(0)\n",
    "        self.df = self.df.ffill()\n",
    "\n",
    "    # Create percent change columns for prices\n",
    "    def create_pct_columns(self):\n",
    "        price_fields = ['Open', 'High', 'Low', 'Close']\n",
    "        price_df = self.df[price_fields]\n",
    "        pct_df = price_df.pct_change()\n",
    "        l0_fields = pct_df.columns.get_level_values(0)\n",
    "        l1_fields = pct_df.columns.get_level_values(1)\n",
    "        l0_fields = [s+'_pct' for s in l0_fields]\n",
    "        pct_df.columns = pd.MultiIndex.from_arrays(\n",
    "            [l0_fields, l1_fields], names=pct_df.columns.names)\n",
    "        self.df = pd.concat([self.df, pct_df], axis=1).sort_index(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4181036",
   "metadata": {},
   "source": [
    "# Create Reddit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5d11246",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = RedditData(years, subreddits, stock_dict, praw_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82944a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data.load_compressed_ndjson(compressed_data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b30709",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data.load_api_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f728015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>stock</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46cgr4</td>\n",
       "      <td>1.455762e+09</td>\n",
       "      <td>Oranguthang</td>\n",
       "      <td>First option trade: 10 NVDA @28 3/4 expiry. No...</td>\n",
       "      <td>I paid for the extra week, when does time real...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2016-02-18 02:21:58+00:00</td>\n",
       "      <td>nvidia</td>\n",
       "      <td>options</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4iamkx</td>\n",
       "      <td>1.462639e+09</td>\n",
       "      <td>RTiger</td>\n",
       "      <td>earnings week 5/9+</td>\n",
       "      <td>Earnings season is winding down.\\n\\nA few nota...</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2016-05-07 16:29:19+00:00</td>\n",
       "      <td>nvidia</td>\n",
       "      <td>options</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5cfsb4</td>\n",
       "      <td>1.478886e+09</td>\n",
       "      <td>irishtrader</td>\n",
       "      <td>NVDA - Huge option move discrepancies - reason?</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-11-11 17:45:52+00:00</td>\n",
       "      <td>nvidia</td>\n",
       "      <td>options</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5dzin9</td>\n",
       "      <td>1.479669e+09</td>\n",
       "      <td>DeadL0cked</td>\n",
       "      <td>What am I missing here? This is a vertical put...</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>2016-11-20 19:03:25+00:00</td>\n",
       "      <td>nvidia</td>\n",
       "      <td>options</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5jed7p</td>\n",
       "      <td>1.482254e+09</td>\n",
       "      <td>jstubb</td>\n",
       "      <td>Help explaining a Call Spread</td>\n",
       "      <td>Ok, I just came across something I haven't see...</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-12-20 17:06:45+00:00</td>\n",
       "      <td>nvidia</td>\n",
       "      <td>options</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   created_utc       author  \\\n",
       "0  46cgr4  1.455762e+09  Oranguthang   \n",
       "1  4iamkx  1.462639e+09       RTiger   \n",
       "2  5cfsb4  1.478886e+09  irishtrader   \n",
       "3  5dzin9  1.479669e+09   DeadL0cked   \n",
       "4  5jed7p  1.482254e+09       jstubb   \n",
       "\n",
       "                                               title  \\\n",
       "0  First option trade: 10 NVDA @28 3/4 expiry. No...   \n",
       "1                                 earnings week 5/9+   \n",
       "2    NVDA - Huge option move discrepancies - reason?   \n",
       "3  What am I missing here? This is a vertical put...   \n",
       "4                      Help explaining a Call Spread   \n",
       "\n",
       "                                            selftext  score  num_comments  \\\n",
       "0  I paid for the extra week, when does time real...      1            13   \n",
       "1  Earnings season is winding down.\\n\\nA few nota...      6             7   \n",
       "2                                                         3             5   \n",
       "3                                                         3            13   \n",
       "4  Ok, I just came across something I haven't see...      6             5   \n",
       "\n",
       "                  timestamp   stock subreddit query  \n",
       "0 2016-02-18 02:21:58+00:00  nvidia   options   NaN  \n",
       "1 2016-05-07 16:29:19+00:00  nvidia   options   NaN  \n",
       "2 2016-11-11 17:45:52+00:00  nvidia   options   NaN  \n",
       "3 2016-11-20 19:03:25+00:00  nvidia   options   NaN  \n",
       "4 2016-12-20 17:06:45+00:00  nvidia   options   NaN  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_data.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297de630",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(reddit_data.df, pd.DataFrame)\n",
    "reddit_data.df.to_csv(reddit_csv_path, encoding='utf-8', index=True, header=True, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9861e537",
   "metadata": {},
   "source": [
    "# Create stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12d7c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  6 of 6 completed\n"
     ]
    }
   ],
   "source": [
    "stock_data = StockData(tickers, years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b8eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.impute_off_days()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2addcbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.create_pct_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.df.to_csv(stock_csv_path, encoding='utf-8', index=True, header=True, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
