{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e11cbac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!usr/bin/env python3\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import praw\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e69cc44",
   "metadata": {},
   "source": [
    "# Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6f19d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path.cwd()\n",
    "compressed_data_root = root / 'posts'\n",
    "reddit_csv_path = root / '10y_reddit_data.csv.zst'\n",
    "stock_csv_path = root / '10y_stock_data.csv.zst'\n",
    "\n",
    "years=10\n",
    "\n",
    "subreddits = [\"stocks\", \"StockMarket\", \"investing\", \"wallstreetbets\", \"options\", \"trading\"]\n",
    "\n",
    "stock_dict = {\n",
    "    'nvidia': ['nvda', 'nvidia'],\n",
    "    'tesla': ['tsla', 'tesla'],\n",
    "    'apple': ['aapl', 'apple'],\n",
    "    'amazon': ['amzn', 'amazon'],\n",
    "    'microsoft': ['msft', 'microsoft'],\n",
    "    'google': ['googl', 'google', 'alphabet']\n",
    "}\n",
    "\n",
    "tickers = ['nvda', 'tsla', 'aapl', 'amzn', 'msft', 'googl']\n",
    "\n",
    "praw_api = praw.Reddit(\n",
    "            client_id=\"5uFqCBUPadVnIxKHG0hnhw\",\n",
    "            client_secret=\"LDFyai0bjEAkEQqo5joU7PjSjtq2eQ\",\n",
    "            user_agent=\"TeslaScraper:v1.0 (by u/RecognitionSame5433)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb1bf4",
   "metadata": {},
   "source": [
    "# Reddit data fields\n",
    "- INDEX: Day normalized timestamp\n",
    "- id\n",
    "- created_utc\n",
    "- author\n",
    "- title\n",
    "- selftext\n",
    "- score\n",
    "- num_comments\n",
    "- query (only for api data)\n",
    "- stock\n",
    "- subreddit\n",
    "- source (api or archive)\n",
    "\n",
    "# Stock data fields per ticker\n",
    "- INDEX: Day normalized timestamp\n",
    "- Open\n",
    "- High\n",
    "- Low\n",
    "- Close\n",
    "- Volume\n",
    "- Open_pct\n",
    "- High_pct\n",
    "- Low_pct\n",
    "- Close_pct\n",
    "- *Revenue\n",
    "- *Earnings\n",
    "- *Revenue_pct\n",
    "- *Earnings_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3056ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 154 (1586326294.py, line 157)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 157\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mclass StockData:\u001b[39m\n                    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after function definition on line 154\n"
     ]
    }
   ],
   "source": [
    "class RedditData:\n",
    "    def __init__(self, years, subreddits, stock_dict, praw_api):\n",
    "        self.subreddits = subreddits\n",
    "        self.stock_dict = stock_dict\n",
    "        self.api = praw_api\n",
    "        self.timedelta = pd.Timedelta(days=365 * years)\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "    def search_subreddit(self, subreddit_name:str, query:str):\n",
    "        subreddit = self.api.subreddit(subreddit_name)\n",
    "        gen = subreddit.search(query, sort='new', time_filter='year', limit=1000)\n",
    "        rows = []\n",
    "        for s in gen:\n",
    "            rows.append({\n",
    "                    \"id\": s.id,\n",
    "                    \"created_utc\" : s.created_utc,\n",
    "                    \"author\": str(s.author) if s.author else None,\n",
    "                    \"title\": s.title,\n",
    "                    \"selftext\": s.selftext,\n",
    "                    \"score\": s.score,\n",
    "                    \"num_comments\": s.num_comments,\n",
    "                })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    # Get year of data for all subreddits and queries\n",
    "    def load_api_data(self):\n",
    "        df_list = []\n",
    "        all_query_pairs = [(stock, query)\n",
    "                           for stock, query_list in self.stock_dict.items()\n",
    "                           for query in query_list\n",
    "                           ]\n",
    "        for stock, query in tqdm(all_query_pairs, desc='Queries', position=0):\n",
    "            for sub in tqdm(self.subreddits,\n",
    "                            desc='Subreddits',\n",
    "                            leave=False,\n",
    "                            position=1):\n",
    "                df = self.search_subreddit(sub, query)\n",
    "                df['timestamp'] = (\n",
    "                    pd.to_datetime(df['created_utc'], unit='s', utc=True)\n",
    "                    .dt.normalize()\n",
    "                )\n",
    "                df.set_index('timestamp')\n",
    "                df['query'] = query\n",
    "                df['stock'] = stock\n",
    "                df['subreddit'] = sub\n",
    "                df_list.append(df)\n",
    "        nonempty_dfs = [df for df in df_list if not df.empty]\n",
    "        print(f'{len(nonempty_dfs)} out of {len(df_list)} queries had hits')\n",
    "        new_df = pd.concat(df_list)\n",
    "        self.add_data(new_df, 'api')\n",
    "        print('API data loaded')\n",
    "\n",
    "    # Parses all compressed data root\n",
    "    def load_compressed_ndjson(self, root: Path):\n",
    "        df_list = []\n",
    "        for file_path in root.rglob('*.zst'):\n",
    "            print(f'Extracting {file_path.name}')\n",
    "            subreddit = file_path.stem.removesuffix('_submissions')\n",
    "            chunk_iter = pd.read_json(\n",
    "                file_path,\n",
    "                lines=True,\n",
    "                compression='zstd',\n",
    "                chunksize=2**16\n",
    "            )\n",
    "            df_chunks = []\n",
    "            # Process and append each chunk\n",
    "            for df_chunk in tqdm(chunk_iter,\n",
    "                                desc='File chunks processed',\n",
    "                                leave=False):\n",
    "                df_chunk['timestamp'] = (\n",
    "                    pd.to_datetime(df_chunk['created_utc'], unit='s', utc=True)\n",
    "                    .dt.normalize()\n",
    "                )\n",
    "                df_chunk.set_index('timestamp')\n",
    "                df_chunk = self.restrict_time_for(df_chunk)\n",
    "                df_chunk = self.restrict_columns_for(df_chunk)\n",
    "                df_chunk = self.keyword_filter_for(df_chunk)\n",
    "                df_chunks.append(df_chunk)\n",
    "            if not df_chunks:\n",
    "                raise RuntimeError('File yielded no rows')\n",
    "            df = pd.concat(df_chunks)\n",
    "            df = self.drop_duplicates_for(df)\n",
    "            df['subreddit'] = subreddit\n",
    "            df_list.append(df)\n",
    "        if not df_list:\n",
    "            raise RuntimeError('Failed to load any data from files')\n",
    "        new_df = pd.concat(df_list)\n",
    "        self.add_data(new_df, 'archive')\n",
    "        print('Compressed data loaded')\n",
    "    \n",
    "    # Remove duplicates and check for overlap when necessary\n",
    "    def add_data(self, df, source):\n",
    "        df = self.drop_duplicates_for(df)\n",
    "        df = self.drop_invalid_posts_for(df)\n",
    "        df['source'] = source\n",
    "        if self.df.empty:\n",
    "            self.df = df\n",
    "        else:\n",
    "            new_df = pd.concat([self.df, df])\n",
    "            overlap = new_df.duplicated(subset=['id', 'stock']).sum()\n",
    "            if overlap == 0:\n",
    "                raise ValueError('Time ranges do not overlap')\n",
    "            print(f'Dataframe overlap of {overlap} rows')\n",
    "            self.df = self.drop_duplicates_for(new_df)\n",
    "\n",
    "    def keyword_filter_for(self, df):\n",
    "        keyword_cols = ['title', 'selftext']\n",
    "        text_df = df[keyword_cols]\n",
    "        df_list = []\n",
    "        for stock, query_list in self.stock_dict.items():\n",
    "            pattern = '|'.join(re.escape(q) for q in query_list)\n",
    "            mask = (\n",
    "                text_df\n",
    "                .apply(lambda col: col.astype(str).str.contains(pattern, case=False, na=False))\n",
    "                .any(axis=1)\n",
    "            )\n",
    "            df_part = df[mask].copy()\n",
    "            df_part['stock'] = stock\n",
    "            df_list.append(df_part)\n",
    "        if df_list:\n",
    "            new_df = pd.concat(df_list)\n",
    "            new_df = self.drop_duplicates_for(new_df)\n",
    "        else:\n",
    "            new_df = df.iloc[0:0].copy()\n",
    "        return new_df\n",
    "    \n",
    "    def drop_invalid_posts_for(self, df):\n",
    "        keywords = ['[removed]', '[deleted]']\n",
    "        pattern = '|'.join([re.escape(s) for s in keywords])\n",
    "        mask = (\n",
    "            df.selftext.str.contains(pattern) |\n",
    "            df.title.str.contains(pattern)\n",
    "        )\n",
    "        new_df = df[~mask]\n",
    "        return new_df\n",
    "            \n",
    "    def restrict_columns_for(self, df):\n",
    "        reddit_columns = ['id', 'created_utc', 'author', 'title', 'selftext', 'score', 'num_comments']\n",
    "        new_df = df[reddit_columns].copy()\n",
    "        return new_df\n",
    "\n",
    "    def restrict_time_for(self, df):\n",
    "        if not str(df.index.dtype) == \"datetime64[ns, UTC]\":\n",
    "            raise RuntimeError('Invalid index type: expected datetime64[ns, UTC]')\n",
    "        start_timestamp = pd.Timestamp.utcnow() - self.timedelta\n",
    "        new_df = df.sort_index().loc[start_timestamp:]\n",
    "        return new_df\n",
    "\n",
    "    # Remove duplicates for (id, stock) and enforce timeframe\n",
    "    def drop_duplicates_for(self, df):\n",
    "        new_df = df.drop_duplicates(subset=['id', 'stock'], inplace=False)\n",
    "        return new_df\n",
    "    \n",
    "    def load_csv_data(self):\n",
    "        self.df = pd.read_csv('10y_reddit_data.csv.zst', encoding='utf-8')\n",
    "\n",
    "\n",
    "class StockData:\n",
    "    def __init__(self, tickers, years, interval='1d'):\n",
    "        self.tickers = tickers\n",
    "        df = yf.download(tickers, period=f'{years}y', interval=interval, auto_adjust=True)\n",
    "        if df is None:\n",
    "            raise ValueError('Data failed to download')\n",
    "        self.df = df\n",
    "\n",
    "    # Forward fill OHLC and zero Volume\n",
    "    def impute_off_days(self):\n",
    "        full_idx = pd.date_range(self.df.index.min(), self.df.index.max(), freq='D')\n",
    "        self.df = self.df.reindex(full_idx)\n",
    "        self.df['Volume'] = self.df['Volume'].fillna(0)\n",
    "        self.df = self.df.ffill()\n",
    "\n",
    "    # Create percent change columns for prices\n",
    "    def create_pct_columns(self):\n",
    "        price_fields = ['Open', 'High', 'Low', 'Close']\n",
    "        price_df = self.df[price_fields]\n",
    "        pct_df = price_df.pct_change()\n",
    "        l0_fields = pct_df.columns.get_level_values(0)\n",
    "        l1_fields = pct_df.columns.get_level_values(1)\n",
    "        l0_fields = [s+'_pct' for s in l0_fields]\n",
    "        pct_df.columns = pd.MultiIndex.from_arrays(\n",
    "            [l0_fields, l1_fields], names=pct_df.columns.names)\n",
    "        self.df = pd.concat([self.df, pct_df], axis=1).sort_index(axis=1)\n",
    "\n",
    "    def load_csv(self):\n",
    "        self.df = pd.read_csv('10y_stock_data.csv.zst', header=[0, 1], index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4181036",
   "metadata": {},
   "source": [
    "# Create Reddit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d11246",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = RedditData(years, subreddits, stock_dict, praw_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82944a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data.load_compressed_ndjson(compressed_data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b30709",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data.load_api_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ad352",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((reddit_data.df['source'] == 'api').sum())\n",
    "print((reddit_data.df['source'] == 'archive').sum())\n",
    "api_df = reddit_data.df[reddit_data.df['source'] == 'api']\n",
    "arc_df = reddit_data.df[reddit_data.df['source'] == 'archive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80a600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reddit_data.df['id'].nunique())\n",
    "print(api_df['id'].nunique())\n",
    "print(arc_df['id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864380c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 100_000, 100):\n",
    "    selftext = arc_df.iloc[i].selftext.lower()\n",
    "    title = arc_df.iloc[i].title.lower()\n",
    "    stock = arc_df.iloc[i].stock\n",
    "    if len(selftext) < 30:\n",
    "        # print(title)\n",
    "        print(selftext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f872090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['[removed]', '[deleted]']\n",
    "pattern = '|'.join([re.escape(s) for s in keywords])\n",
    "print(pattern)\n",
    "mask = (\n",
    "    arc_df.selftext.str.contains(pattern) |\n",
    "    arc_df.title.str.contains(pattern)\n",
    ")\n",
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297de630",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data.df.to_csv(reddit_csv_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9861e537",
   "metadata": {},
   "source": [
    "# Create stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12d7c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  6 of 6 completed\n"
     ]
    }
   ],
   "source": [
    "stock_data = StockData(tickers, years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6b8eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.impute_off_days()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2addcbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.create_pct_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.df.to_csv(stock_csv_path, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
