{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e11cbac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!usr/bin/env python3\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import praw\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e69cc44",
   "metadata": {},
   "source": [
    "# Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6f19d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path.cwd()\n",
    "compressed_data_root = root / 'posts'\n",
    "reddit_csv_path = root.parent / '10y_reddit_data.csv'\n",
    "stock_csv_path = root.parent / '10y_stock_data.csv'\n",
    "\n",
    "years=10\n",
    "\n",
    "subreddits = [\"stocks\", \"StockMarket\", \"investing\", \"wallstreetbets\", \"options\", \"trading\"]\n",
    "\n",
    "stock_dict = {\n",
    "    'nvidia': ['nvda', 'nvidia'],\n",
    "    'tesla': ['tsla', 'tesla'],\n",
    "    'apple': ['aapl', 'apple'],\n",
    "    'amazon': ['amzn', 'amazon'],\n",
    "    'microsoft': ['msft', 'microsoft'],\n",
    "    'google': ['googl', 'google', 'alphabet']\n",
    "}\n",
    "\n",
    "tickers = ['nvda', 'tsla', 'aapl', 'amzn', 'msft', 'googl']\n",
    "\n",
    "praw_api = praw.Reddit(\n",
    "            client_id=\"5uFqCBUPadVnIxKHG0hnhw\",\n",
    "            client_secret=\"LDFyai0bjEAkEQqo5joU7PjSjtq2eQ\",\n",
    "            user_agent=\"TeslaScraper:v1.0 (by u/RecognitionSame5433)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb1bf4",
   "metadata": {},
   "source": [
    "# Reddit data fields\n",
    "- id\n",
    "- created_utc\n",
    "- timestamp\n",
    "- author\n",
    "- title\n",
    "- selftext\n",
    "- score\n",
    "- num_comments\n",
    "- query (only for api data)\n",
    "- stock\n",
    "- subreddit\n",
    "- source (api or archive)\n",
    "\n",
    "# Stock data fields per ticker\n",
    "- Open\n",
    "- High\n",
    "- Low\n",
    "- Close\n",
    "- Volume\n",
    "- Open_pct\n",
    "- High_pct\n",
    "- Low_pct\n",
    "- Close_pct\n",
    "- Revenue\n",
    "- Earnings\n",
    "- Revenue\n",
    "- Earnings\n",
    "- Revenue_pct\n",
    "- Earnings_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3056ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditData:\n",
    "    def __init__(self, years, subreddits, stock_dict, praw_api):\n",
    "        self.subreddits = subreddits\n",
    "        self.stock_dict = stock_dict\n",
    "        self.api = praw_api\n",
    "        self.timedelta = pd.Timedelta(days=365 * years)\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "    def search_subreddit(self, subreddit_name:str, query:str):\n",
    "        subreddit = self.api.subreddit(subreddit_name)\n",
    "        gen = subreddit.search(query, sort='new', time_filter='year', limit=1000)\n",
    "        rows = []\n",
    "        for s in gen:\n",
    "            rows.append({\n",
    "                    \"id\": s.id,\n",
    "                    \"created_utc\" : s.created_utc,\n",
    "                    \"timestamp\": pd.to_datetime(s.created_utc, unit='s', utc=True),\n",
    "                    \"author\": str(s.author) if s.author else None,\n",
    "                    \"title\": s.title,\n",
    "                    \"selftext\": s.selftext,\n",
    "                    \"score\": s.score,\n",
    "                    \"num_comments\": s.num_comments,\n",
    "                })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    # Get year of data for all subreddits and queries\n",
    "    def load_api_data(self):\n",
    "        df_list = []\n",
    "        all_query_pairs = [(stock, query)\n",
    "                           for stock, query_list in self.stock_dict.values()\n",
    "                           for query in query_list\n",
    "                           ]\n",
    "        for stock, query in tqdm(all_query_pairs, desc='Queries', position=0):\n",
    "            for sub in tqdm(self.subreddits,\n",
    "                            desc='Subreddits',\n",
    "                            leave=False,\n",
    "                            position=1):\n",
    "                df = self.search_subreddit(sub, query)\n",
    "                df['query'] = query\n",
    "                df['stock'] = stock\n",
    "                df['subreddit'] = sub\n",
    "                df_list.append(df)\n",
    "        nonempty_dfs = [df for df in df_list if not df.empty]\n",
    "        print(f'{len(nonempty_dfs)} out of {len(df_list)} queries had hits')\n",
    "        new_df = pd.concat(df_list, ignore_index=True)\n",
    "        self.add_data(new_df, 'api')\n",
    "        print('API data loaded')\n",
    "\n",
    "    # Parses all compressed data root\n",
    "    def load_compressed_ndjson(self, root: Path):\n",
    "        df_list = []\n",
    "        for file_path in root.rglob('*.zst'):\n",
    "            print(f'Extracting {file_path.name}')\n",
    "            subreddit = file_path.stem.removesuffix('_submissions')\n",
    "            chunk_iter = pd.read_json(\n",
    "                file_path,\n",
    "                lines=True,\n",
    "                compression='zstd',\n",
    "                chunksize=2**16\n",
    "            )\n",
    "            df_chunks = []\n",
    "            # Process and append each chunk\n",
    "            for df_chunk in tqdm(chunk_iter,\n",
    "                                desc='File chunks processed',\n",
    "                                leave=False):\n",
    "                df_chunk = self.restrict_columns_for(df_chunk)\n",
    "                df_chunk['timestamp'] = pd.to_datetime(df_chunk['created_utc'], unit='s', utc=True)\n",
    "                df_chunk = self.restrict_time_for(df_chunk)\n",
    "                df_chunk = self.keyword_filter_for(df_chunk)\n",
    "                df_chunks.append(df_chunk)\n",
    "            if not df_chunks:\n",
    "                raise RuntimeError('File yielded no rows')\n",
    "            df = pd.concat(df_chunks, ignore_index=True)\n",
    "            df = self.drop_duplicates_for(df)\n",
    "            df['subreddit'] = subreddit\n",
    "            df_list.append(df)\n",
    "        if not df_list:\n",
    "            raise RuntimeError('Failed to load any data from files')\n",
    "        new_df = pd.concat(df_list, ignore_index=True)\n",
    "        self.add_data(new_df, 'archive')\n",
    "        print('Compressed data loaded')\n",
    "    \n",
    "    # Remove duplicates and check for overlap when necessary\n",
    "    def add_data(self, df, source):\n",
    "        df = self.drop_duplicates_for(df)\n",
    "        df = self.drop_invalid_posts_for(df)\n",
    "        df['source'] = source\n",
    "        if self.df.empty:\n",
    "            self.df = df\n",
    "        else:\n",
    "            new_df = pd.concat([self.df, df], ignore_index=True)\n",
    "            overlap = new_df.duplicated(subset=['id', 'stock']).sum()\n",
    "            if overlap == 0:\n",
    "                raise ValueError('Time ranges do not overlap')\n",
    "            print(f'Dataframe overlap of {overlap} rows')\n",
    "            self.df = self.drop_duplicates_for(new_df)\n",
    "\n",
    "    def keyword_filter_for(self, df):\n",
    "        keyword_cols = ['title', 'selftext']\n",
    "        text_df = df[keyword_cols]\n",
    "        df_list = []\n",
    "        for stock, query_list in self.stock_dict.items():\n",
    "            pattern = '|'.join(re.escape(q) for q in query_list)\n",
    "            mask = (\n",
    "                text_df\n",
    "                .apply(lambda col: col.astype(str).str.contains(pattern, case=False, na=False))\n",
    "                .any(axis=1)\n",
    "            )\n",
    "            df_part = df[mask].copy()\n",
    "            df_part['stock'] = stock\n",
    "            df_list.append(df_part)\n",
    "        if df_list:\n",
    "            new_df = pd.concat(df_list, ignore_index=True)\n",
    "            new_df = self.drop_duplicates_for(new_df)\n",
    "        else:\n",
    "            new_df = df.iloc[0:0].copy()\n",
    "        return new_df\n",
    "    \n",
    "    def drop_invalid_posts_for(self, df):\n",
    "        keywords = ['[removed]', '[deleted]']\n",
    "        pattern = '|'.join([re.escape(s) for s in keywords])\n",
    "        mask = (\n",
    "            df.selftext.str.contains(pattern) |\n",
    "            df.title.str.contains(pattern)\n",
    "        )\n",
    "        new_df = df[~mask]\n",
    "        return new_df\n",
    "            \n",
    "    def restrict_columns_for(self, df):\n",
    "        reddit_columns = ['id', 'created_utc', 'author', 'title', 'selftext', 'score', 'num_comments']\n",
    "        new_df = df[reddit_columns].copy()\n",
    "        return new_df\n",
    "\n",
    "    def restrict_time_for(self, df):\n",
    "        start_timestamp = pd.Timestamp.utcnow() - self.timedelta\n",
    "        new_df = df[df.timestamp >= start_timestamp].copy()\n",
    "        return new_df\n",
    "\n",
    "    # Remove duplicates for (id, stock) and enforce timeframe\n",
    "    def drop_duplicates_for(self, df):\n",
    "        new_df = df.drop_duplicates(subset=['id', 'stock'], inplace=False)\n",
    "        return new_df\n",
    "\n",
    "\n",
    "class StockData:\n",
    "    def __init__(self, tickers, years, interval='1d'):\n",
    "        self.tickers = tickers\n",
    "        df = yf.download(tickers, period=f'{years}y', interval=interval, auto_adjust=True)\n",
    "        if df is None:\n",
    "            raise ValueError('Data failed to download')\n",
    "        self.df = df\n",
    "\n",
    "    # Forward fill OHLC and zero Volume\n",
    "    def impute_off_days(self):\n",
    "        full_idx = pd.date_range(self.df.index.min(), self.df.index.max(), freq='D')\n",
    "        self.df = self.df.reindex(full_idx)\n",
    "        self.df['Volume'] = self.df['Volume'].fillna(0)\n",
    "        self.df = self.df.ffill()\n",
    "\n",
    "    # Create percent change columns for prices\n",
    "    def create_pct_columns(self):\n",
    "        price_fields = ['Open', 'High', 'Low', 'Close']\n",
    "        price_df = self.df[price_fields]\n",
    "        pct_df = price_df.pct_change()\n",
    "        l0_fields = pct_df.columns.get_level_values(0)\n",
    "        l1_fields = pct_df.columns.get_level_values(1)\n",
    "        l0_fields = [s+'_pct' for s in l0_fields]\n",
    "        pct_df.columns = pd.MultiIndex.from_arrays(\n",
    "            [l0_fields, l1_fields], names=pct_df.columns.names)\n",
    "        self.df = pd.concat([self.df, pct_df], axis=1).sort_index(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4181036",
   "metadata": {},
   "source": [
    "# Create Reddit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c5d11246",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = RedditData(years, subreddits, stock_dict, praw_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "82944a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting options_submissions.zst\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abfd1209b7d3477d9e0b60ad0af46934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing file: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting wallstreetbets__submissions.zst\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00135dfd752424f8015fb03afa98cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing file: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting stocks_submissions.zst\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b98748336848c4b234b5b536e558a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing file: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Trading_submissions.zst\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc7c53f973443eb99cf0e9ea846d930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing file: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting investing_submissions.zst\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee975b5bbf445ccb717c00d809c67c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing file: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting StockMarket_submissions.zst\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c90d05ed38654610bac6237b79de71e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing file: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting wallstreetbets_submissions.zst\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa3a12f55914f34a727e99b5ea41831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing file: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed data loaded\n"
     ]
    }
   ],
   "source": [
    "reddit_data.load_compressed_ndjson(compressed_data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "02b30709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d7b3a288fb4ae0b1a6c55db63a14e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883441302bff49dc82bafabdad94fc38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f0f3856032462a9a84ec0612f207cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f2b58cea814353b64101f99b2245ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42cf3b82f94045988eaff4568f37ca9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29155e15932142e2bdcd9476387351d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5266ddbfc7434f51b68108006bbff70d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d031312354d548afbedf9420f49192aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e007342bb51d4d088b3c2cba8a991af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96ecda9a41c4c548ce7a908ddc84745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c886f26836dc48b59636189a66484771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c59366a0ff49f9bf7efabe1a4a7720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077df1372b424d06a5163cb20da3edab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1a585bf93143fd833fdd9185de86b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44af57bf4a1420dadca615933ea7660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00246175fc734707abdf192d6e9eccc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb588b6d0a5493eae56d351427674ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33bd67e8868413ea6ef0d719a8fd141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447c75166cd24490b861261ff20cec8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 out of 78 queries had hits\n",
      "Dataframe overlap of 400 rows\n",
      "API data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1245/3705962009.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['source'] = source\n"
     ]
    }
   ],
   "source": [
    "reddit_data.load_api_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f728015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233738"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reddit_data.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e64ad352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8993\n",
      "224745\n"
     ]
    }
   ],
   "source": [
    "print((reddit_data.df['source'] == 'api').sum())\n",
    "print((reddit_data.df['source'] == 'archive').sum())\n",
    "api_df = reddit_data.df[reddit_data.df['source'] == 'api']\n",
    "arc_df = reddit_data.df[reddit_data.df['source'] == 'archive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b80a600b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191177\n",
      "6219\n",
      "184972\n"
     ]
    }
   ],
   "source": [
    "print(reddit_data.df['id'].nunique())\n",
    "print(api_df['id'].nunique())\n",
    "print(arc_df['id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864380c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 100_000, 1_000):\n",
    "    selftext = arc_df.iloc[i].selftext.lower()\n",
    "    title = arc_df.iloc[i].title.lower()\n",
    "    stock = arc_df.iloc[i].stock\n",
    "    if len(selftext) < 30:\n",
    "        # print(title)\n",
    "        print(selftext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f872090a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\[removed\\]|\\[deleted\\]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(64946)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = ['[removed]', '[deleted]']\n",
    "pattern = '|'.join([re.escape(s) for s in keywords])\n",
    "print(pattern)\n",
    "mask = (\n",
    "    arc_df.selftext.str.contains(pattern) |\n",
    "    arc_df.title.str.contains(pattern)\n",
    ")\n",
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297de630",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data.df.to_csv(reddit_csv_path, encoding='utf-8', index=True, header=True, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9861e537",
   "metadata": {},
   "source": [
    "# Create stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d7c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = StockData(tickers, years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b8eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.impute_off_days()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2addcbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.create_pct_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.df.to_csv(stock_csv_path, encoding='utf-8', index=True, header=True, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
